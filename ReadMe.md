# Simple neural nets
## The simplest networks around
## Training
Trained using a method very similar to gradient descent, where I test the loss of the result of adding  `learning rate` to each weight, and if loss is less, add learning rate.
It's like gradient descent, but no calculus (which I don't understand).