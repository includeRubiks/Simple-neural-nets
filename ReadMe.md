# Simple neural nets
## The simplest networks around
## Training
Trained using a method very similar to gradient descent, where I add `learning rate` to each weight, then test, 
see if the loss is less, and if it is, keep that weight that way, otherwise, subtract `learning rate`.
It's like gradient descent, but no calculus (which I don't understand).